\documentclass[11pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.6in}
\linespread{1.3}

% \renewcommand{\labelenumi}{\alph{enumi}}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\usepackage{eufrak}


\newcommand{\spacer}{\displaystyle{\phantom{\int}}}
\newcommand{\blank}[1]{\underline{\hspace{#1}}}
\newcommand{\diff}[2]{\frac{d#1}{d#2}}
\newcommand{\difftwo}[2]{\frac{d^2#1}{d{#2}^2}}
\newcommand\dd[2]{\frac{\partial{#1}}{\partial{#2}}}
\newenvironment{piecewise}{\left\{\begin{array}{l@{,\quad}l}}{\end{array}\right.}
\newcommand{\col}[3]{\begin{bmatrix} #1 \\ #2 \\ #3 \end{bmatrix}}
\newcommand{\coll}[4]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4\end{bmatrix}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\adj}{adj}
\newcommand{\points}[1]{({\it #1 points})}
\newcommand{\I}{\mathbb{I}}
\newcommand{\F}{\mathfrak{F}}
\newcommand{\D}{\mathfrak{D}}
\newcommand{\M}{\mathcal{M}}
\def\dx{\Delta x}
\def\dsst{\displaystyle}
\hyphenation{}

% \epsfxsize=3.25in \epsfbox{line.eps}

\begin{document}
\noindent \textbf{Duke University  \hfill  October 25, 2016} \\
\line(1,0){455}
    
\begin{center}
\Large{{\bf Thesis Thoughts/Intro/Ideas}}\\
\end{center}

The current state of AI/technology indicates the need to imbue machines with the ability to make moral decisions. Self driving cars provide an excellent domain to study -- they are already on our streets, yet we as a society have not taken any significant steps towards deciding how they will handle moral dilemmas, such as the ``trolley car'' problem. 

There are many ethical and moral issues associated with programming self driving cars to make moral decisions, the obvious one being ``what decisions should they make when''? However, there are also some non-obvious yet non-trivial ethical issues here.

\begin{itemize}
\item Who gets to program the cars? Is it an individual, a private company, a government?
\item It is well-known that algorithms can exhibit biases (for example, racial). Should/how will algorithms account for this bias?
\item What degree of ownership will society have over the programs/decision-making algorithms in these cars? 
\item Should people have a right to voice their inputs for self-driving car (and other moral AI) programs? Should these programs be designed by researchers/governments to maximize social utility, or, should people be able to``vote'' on the moral preferences?
\end{itemize}
One way to implement a moral AI would be to try to somehow abstract or understand moral features/principles, and build an algorithm that uses this information to reason morally. For instance, you might survey people on which moral features they feel indicate whether a decision is right or wrong, and then build an algorithm that, using those features, makes decisions given new inputs. There are some potential problems with this approach:

\begin{itemize}
\item Voter/citizen distrust -- people are less likely to support/adopt a system they do not understand (think: machines taking over the world), and a system that abstracts moral values may be harder to understand.

\item Consistency issues -- say we survey people on a problem X and use the results of that survey to build a new system, which then decides on problem Y. Is that the same as the majority vote decision the people would have made if they had originally been asked about problem Y?
\end{itemize}
One potential solution to this problem is via voting. Consider this: what if, every time a moral decision needed to be made, we conducted a vote, asking each person what action should be taken, and using a voting algorithm to decide what to do. This would seem to solve both problems -- people are (generally) likely to trust systems in which votes make decisions (our current government) and the consistency issue is avoided. This also seems to be grounded in current democratic philosophy in which votes are used to make decisions.

Of course, it is not possible to conduct a vote each time a decision needs to be made (due to potentially $>$ millions of machines making many decisions all the time). We can, however, conceive of programs that can cast votes for people. For instance, imagine a moral problem with well-defined features. We could have a classifier that, for each person, learns their moral preferences and predicts their view on the moral problem. Then, when we need to make a decision, we could query all the classifiers, treat them as votes, and aggregate them into one decision. 

Already this is an interesting problem for its applications to moral AI decision making and society-in-the-loop computing (as discussed by Iyad Rahwan). However, it is not clear that an algorithm will always have the time to evaluate all classifiers. Situations on the road are dynamic  -- it seems that sometimes, a self-driving car will have very little time with which to make a decision. It seems that we need to develop methods by which computers can either efficiently evaluate all classifiers, or approximate the result of evaluating all classifiers with high probability. A first attempt to formalize this problem is as follows:

First, we need some way of defining what a moral problem is. 

\theoremstyle{definition}
\begin{definition}{Moral Problem}
A moral problem $\mathfrak{m}$ consists of a domain in which in which a problem can occur, a set of features $\mathfrak{F}$ on which the problem can be evaluated, and a set of decisions $\mathfrak{D}$ that can be made to resolve the problem.
\end{definition}

\theoremstyle{definition}
\begin{definition}{Moral Problem Space}
A moral problem space $\mathcal
{M}$ is the set of all moral problems that share domains, feature sets, and decision sets. Since this is the case we can refer to feature set and decision set of a moral problem space as well.
\end{definition}

\theoremstyle{definition}
\begin{definition}{Decision Function}
Let $\M$ be a moral problem space. Then $F_M: \F \rightarrow \D$ is called a decision function and maps from the feature set of $\M$ to the decision set of $\M$.
\end{definition}

Let's flesh this out by looking at an example of self-driving cars choosing whether to drive straight or swerve to avoid something in the road. In this instance, we can call our domain "self-driving cars", and $\D$ consists of two options: drive straight, or swerve. What does $\F$ consist of? This is highly dependent on what information we are able to gather. In one instance, $\F$ might just consist of the number of people in front of the car, and the number of people to the side of the car. On the other hand, we could envision a more complex scenario where $\F$ contains the number of people who will be hit if a car goes straight, the number of people who will be hit if a car swerves, probabilities of hitting the groups of people, estimated age, estimated race, culpability details (did someone dash into traffic), etc. We could take this one step further -- already, half of U.S. citizens have their faces in law enforcement facial recognition databases. If these could be used to lookup details, we can imagine a situation where a car is able to evaluate a decision based on all sorts of information -- criminal background, disease profile, religion, income, job -- the possible feature set would be almost limitless. \\

Since the definition of $\M$ depends on the feature set, we have that each feature set leads to a different type of moral problem. Thus, the question of whether a car should swerve given knowledge of potential victims' ages is different than the question of whether a car should swerve not given knowledge of potential victims' ages.\\

Now we can turn our attention back to the moral classifier aggregation problem. The problem is as follows: 

\theoremstyle{definition}
\begin{definition}{Moral Aggregation Problem}
Let $\mathcal{F}_M$ be the set of all decision functions on $\M$. The goal in the Moral Aggregation Problem is to develop a procedure that somehow aggegrates the results of applying $f \subset \mathcal{F}_M$ to some problem $m \in \M$ according to some set of evaluation measures.
\end{definition}
Notes:
Need to take into account how certain they are, how certain we are of them.

Use nearest neighbors?

Think about when people just don't know, or do know, or have conflicts. 
There are many ways this could happen:

\begin{itemize}
\item If all decision functions in $\mathcal{F}_M$ are of the same form (ie all decision trees, all SVM, all hyperplanes), then we could exploit the structure via preprocessing to quickly query all functions.

\item We could use random sampling to 

\item More narrowly define problems, such as random sampling

\item bias variance tradeoffs in sampling

\item time tradeoffs in sampling as well

\item if you know how long each algo will take, you can plan / knapsack problem

\item probability you will get the wrong decision, minimize that

\item look at binary decision.

\item distribution over time that an algo will take, you may not know how long (but maybe postpone this)

\item incentives to speed up classifier? does it make it less expressive?

\item correlations between speed and accuracy

\item MAKE ASSUMPTIONS ABOUT WHAT SEems reasonable, make a model/objective from there.

\item tradeoff between strategy and speed.

\item strategy only makes sense when people code up their own algorithms.

\item don't want to bias towards people with simplistic moral tendencies.

\item what to do when there is a correlation between speed and correctness? No correlation -- query all fast functions. If you don't know, maybe query some mostly fast ones, and a few slow ones?

\end{itemize}

\section{Probabilistic Framework}

Let $Y$ represent the function that, given a situation, represents the correct moral decision to make. I assume we can predict $Y$ via an aggregation of decision functions, so $Y = f(X_q) + \epsilon(X)$. In this model, I do not assume each voting function has the same noise value. 
\end{document}
